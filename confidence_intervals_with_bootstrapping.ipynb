{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals for system evaluation using the bootstrapping approach\n",
    "\n",
    "This notebook contains the code needed to create confidence intervals with the bootstrapping approach\n",
    "for (almost) any task, dataset or metric. Four things are required to run this code:\n",
    "\n",
    "* **Test samples**: an array, dataframe or object that can be indexed with an array of integers to select the samples for each bootstrap set. Note that, here, the term sample refers to the output of the system for each original sample input to the system. Those outputs are all that is needed to compute the performance metric.\n",
    "* **Labels**: an array, dataframe or object that can be indexed to select the labels for the samples above.\n",
    "* **Conditions**: an array of integers indicating the conditions of the samples (e.g., the speaker identity). All samples with the same condition will be sampled together when doing bootstrapping.\n",
    "* **Metric**: The metric to be used for assessing performance. A method that takes samples and labels and computes a scalar value. \n",
    "  \n",
    "The samples and the labels can be as simple or as complex as your task requires. For example: \n",
    "\n",
    "* In simple **classification tasks** like emotion recognition, each sample is the vector of scores (or the decision, depending on the metric we want to compute) generated by the system for a given input waveform, the label is the class of the sample, and the condition may be speaker or recording session of the waveform.\n",
    "* In **automatic speech recognition**, each sample is the transcription generated by the system for a given waveform, the label is the ground truth transcription for that waveform, and the condition could be speaker in the waveform. \n",
    "* In **segmentation tasks** like speech activity detection, the sample would be the segmentation generated by the system for a waveform and the label would be the ground truth segmentation. In this case, if the waveforms are chunks extracted from longer waveforms, the condition of a sample should be given by the waveform from which the chunk was extracted.\n",
    "* In **speaker verification**, each sample is a trial (enrollment + test samples) and the label is either \"impostor\" or \"target\". For this task, though, bootstrapping by condition is harder because both sides have different conditions (the speaker or session the waveforms come from). The code in this repository cannot handle this particular case. Instead, joint bootstrapping is needed. Please see the code in the [DCA-PLDA github repository](https://github.com/luferrer/DCA-PLDA/blob/master/dca_plda/scores.py) (compute_performance_with_confidence_intervals method) for an example of how to do joint bootstrapping for speaker verification.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gEIJ0m_bAvI",
    "outputId": "b921aa6c-05d4-4556-bb13-82234f2ea7e9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy dataset. Here the samples are simply the decisions made by the system.\n",
    "\n",
    "decisions, labels, conditions = utils.create_data(200, 200, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval:  0.76   0.86\n"
     ]
    }
   ],
   "source": [
    "# The metric can be changed to any other method that takes a list of samples and labels as input\n",
    "metric =  sklearn.metrics.accuracy_score\n",
    "\n",
    "mvals = []\n",
    "numbootstraps = 1000\n",
    "\n",
    "for nb in np.arange(numbootstraps):\n",
    "    indices = get_bootstrap_indices(N, conditions=conditions)\n",
    "    mvals.append(metric(labels[indices], decisions[indices]))\n",
    "\n",
    "get_conf_int(mvals)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
