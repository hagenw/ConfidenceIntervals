{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Confidence intervals for system evaluation using the bootstrapping approach\n",
                "\n",
                "This notebook contains the code needed to create confidence intervals with the bootstrapping approach\n",
                "for (almost) any task, dataset or metric. Four things are required to run this code:\n",
                "\n",
                "* **Samples**: an array that can be indexed to select the samples for each bootstrap set. Note that, here, the term sample refers to ``whatever is needed to compute the metric of interest''. This will generally be the system's output (scores or decisions). Yet, for metrics that are simple averages of some per-sample loss, the sample can simply be represented by the per-sample loss (more on this below).\n",
                "* **Labels**: an array that can be indexed to select the labels, or any other information needed to compute the metric, for the samples above.\n",
                "* **Conditions**: an array of integers indicating the conditions of the samples (e.g., the speaker identity). All samples with the same condition will be sampled together when doing bootstrapping.\n",
                "* **Metric**: The metric to be used for assessing performance. A method that takes samples and, when necessary, labels and computes a scalar value. \n",
                "  \n",
                "The metric, samples, and labels can be as simple or as complex as your task requires. The table below shows some examples on how the different inputs may be defined. \n",
                "\n",
                "\n",
                "<center>\n",
                "\n",
                "| Task                         | Metric           | Sample                              | Label             |  Condition        |\n",
                "|------------------------------|------------------|-------------------------------------|-------------------|-------------------|\n",
                "| Emotion classification       | Accuracy         | System's decision                   | Emotion label     | Speaker           |\n",
                "| Emotion classification       | Accuracy         | 0 or 1 (correct/incorrect decision) | -                 | Speaker           |\n",
                "| Speaker verification         | EER              | System's score for trial            | Target/Impostor   | See comment below |\n",
                "| Automatic speech recognition | Av. WER          | Per-sample WER                      | -                 | Speaker           |\n",
                "| Automatic speech recognition | Weighted Av. WER | Per-sample WER                      | Num words         | Speaker           |\n",
                "| Diarization                  | Weighted Av. DER | Per-sample DER                      | Num speech frames | Speaker           |\n",
                "\n",
                "</center>\n",
                "\n",
                "Some notes:\n",
                "\n",
                "* For metrics that are averages of some loss over the samples like the accuracy, or the average WER, the sample can be represented directly by the per-sample loss, the metric is simply the average of the loss over the samples, and the label is not needed.\n",
                "* For the weighted average WER and DER metrics often used for ASR and diarization, where the weights are given by the number of words in each sample or the number of speech frames, respectively, the label field can be used to provide the number of words or speech frames for each sample so that the metric can be computed from the individual WER or DER values and this quantity (see example below).\n",
                "* While for speech tasks the speaker is the most common correlation-inducing factor, other factors may exist, like the recording session (if more than one sample is generated in a session) or the original waveform (if samples are waveform chunks extracted from longer waveforms).\n",
                "* In speaker verification, bootstrapping by condition is harder than for other tasks because both sides in a trial (the enrollment and the test side) have their own condition. The code in this repository cannot handle this particular case. Instead, joint bootstrapping is needed. Please see the code in the [DCA-PLDA github repository](https://github.com/luferrer/DCA-PLDA/blob/master/dca_plda/scores.py) (compute_performance_with_confidence_intervals method) for an example of how to do joint bootstrapping for speaker verification.  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting git+https://github.com/luferrer/ConfidenceIntervals.git\n",
                        "  Cloning https://github.com/luferrer/ConfidenceIntervals.git to /private/var/folders/yc/gnzvhlcn0g1gb0kjw3nnj4t80000gn/T/pip-req-build-50vmeskc\n",
                        "  Running command git clone --filter=blob:none --quiet https://github.com/luferrer/ConfidenceIntervals.git /private/var/folders/yc/gnzvhlcn0g1gb0kjw3nnj4t80000gn/T/pip-req-build-50vmeskc\n",
                        "  Resolved https://github.com/luferrer/ConfidenceIntervals.git to commit 2b3fcc8af89a5ac617ab2bb8bc5e84675fc44713\n",
                        "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
                        "\u001b[?25hRequirement already satisfied: scikit-learn in /Users/lferrer/opt/anaconda3/lib/python3.9/site-packages (from confidence-intervals==0.1.0) (1.2.2)\n",
                        "Requirement already satisfied: numpy in /Users/lferrer/opt/anaconda3/lib/python3.9/site-packages (from confidence-intervals==0.1.0) (1.24.1)\n",
                        "Requirement already satisfied: joblib>=1.1.1 in /Users/lferrer/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->confidence-intervals==0.1.0) (1.3.0)\n",
                        "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/lferrer/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->confidence-intervals==0.1.0) (2.2.0)\n",
                        "Requirement already satisfied: scipy>=1.3.2 in /Users/lferrer/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->confidence-intervals==0.1.0) (1.9.1)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install git+https://github.com/luferrer/ConfidenceIntervals.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "3gEIJ0m_bAvI",
                "outputId": "b921aa6c-05d4-4556-bb13-82234f2ea7e9"
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sklearn.metrics\n",
                "from confidence_intervals import evaluate_with_conf_int\n",
                "from confidence_intervals.utils import create_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a toy dataset. Here the samples are simply the decisions made by the system.\n",
                "N0 = 200\n",
                "N1 = 200\n",
                "NC = 20\n",
                "decisions, labels, conditions = create_data(N0, N1, NC, scale=1.0)\n",
                "\n",
                "# Percentage for the confidence interval\n",
                "alpha = 5 \n",
                "\n",
                "# Number of bootstrap samples to use (the run time will be proportional to this number). We set it to\n",
                "# 50/alpha to get enough samples in the tails.\n",
                "num_bootstraps = int(50/alpha)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below we show the simplest way to obtain both the metric on the complete dataset and the confidence interval, in a single line of code.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(0.855, (0.8218000845180959, 0.8952843516058924))"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "metric =  sklearn.metrics.accuracy_score\n",
                "evaluate_with_conf_int(labels, decisions, metric, conditions, num_bootstraps=num_bootstraps, alpha=alpha)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Alternatively, as mentioned in the introduction, one can compute the accuracy as the average of the 0-1 loss. In this case, we need to set the first argument to None, define the second argument as the 0-1 loss, and the metric as the mean. \n",
                "\n",
                "This same approach can be used for ASR or diarization where the metric is an average of per-sample losses that are not quick to compute. In that case, the samples array should contain the per-sample losses, replacing the 0-1 loss used below. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(0.855, (0.8218000845180959, 0.8952843516058924))"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "samples = decisions == labels\n",
                "evaluate_with_conf_int(None, samples, np.average, conditions, num_bootstraps=num_bootstraps, alpha=alpha)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In some cases, the metric is a weighted average of per-sample losses. In that case, the labels array can be used to provide the weight to the metric, which can be defined as below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(0.855, (0.8218000845180959, 0.8952843516058924))"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "samples = decisions == labels\n",
                "\n",
                "# Setting all the weights to 1, the metric is the accuracy, as above. \n",
                "weights = np.ones_like(samples)\n",
                "\n",
                "def metric(weights, samples):\n",
                "    return np.average(samples, weights=weights)\n",
                "\n",
                "evaluate_with_conf_int(weights, samples, metric, conditions, num_bootstraps=num_bootstraps, alpha=alpha)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The block below shows the few lines of code needed to run bootstrapping, unwrapping evaluate_with_conf_int. This shows how straightforward the approach is. The only somewhat complex step is inside the get_bootstrap_indices method for the case that involves conditions. \n",
                "\n",
                "You can use this as pseudo-code if, for example, you need to recode bootstrapping in some other language or compute a more involved metric that does not take only samples and labels as input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.855 (0.8218000845180959, 0.8952843516058924)\n"
                    ]
                }
            ],
            "source": [
                "from confidence_intervals import get_bootstrap_indices, get_conf_int\n",
                "\n",
                "metric =  sklearn.metrics.accuracy_score\n",
                "metric_values = []\n",
                "num_samples = len(decisions)\n",
                "for nb in np.arange(num_bootstraps):\n",
                "    indices = get_bootstrap_indices(num_samples, conditions=conditions, random_state=nb)\n",
                "    metric_values.append(metric(labels[indices], decisions[indices]))\n",
                "\n",
                "print(metric(labels, decisions), get_conf_int(metric_values, alpha))\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below is an example on how to create a bar plot for two systems, with and without using conditions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 300x300 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from confidence_intervals.utils import barplot_with_ci\n",
                "metric =  sklearn.metrics.accuracy_score\n",
                "\n",
                "# The create_data method generates the same labels and conditions for all three cases below. \n",
                "# Only the decisions change, simulating three different systems.\n",
                "decisions = {}\n",
                "decisions['sys1'], labels, conditions = create_data(N0, N1, NC, scale=1.0)\n",
                "decisions['sys2'], labels, conditions = create_data(N0, N1, NC, scale=0.5)\n",
                "decisions['sys3'], labels, conditions = create_data(N0, N1, NC, scale=0.3)\n",
                "\n",
                "data = {}\n",
                "for sys, dec in decisions.items():\n",
                "   val_with_cond = evaluate_with_conf_int(labels, dec, metric, conditions, num_bootstraps=num_bootstraps, alpha=alpha)\n",
                "   val_no_cond   = evaluate_with_conf_int(labels, dec, metric, None,       num_bootstraps=num_bootstraps, alpha=alpha)\n",
                "   data[sys] = {'with cond': val_with_cond, 'without cond': val_no_cond}\n",
                "\n",
                "barplot_with_ci(data, figsize=(3,3))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we show how to create a confidence interval for comparing the performance between two of the systems above. Since the metric needed for this case is a bit more involved than what the evaluate_with_conf_int method can handle, we use the unwrapped code already shown above.\n",
                "\n",
                "**If the confidence interval does not include the value 0.0, then we can reject the null hypothesis that the two systems have the same performance (see Keller, 2005, cited in the README file).**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.10999999999999999 (-0.04750000000000007, -0.015312500000000052)\n"
                    ]
                }
            ],
            "source": [
                "from confidence_intervals import get_bootstrap_indices, get_conf_int\n",
                "\n",
                "base_metric =  sklearn.metrics.accuracy_score\n",
                "\n",
                "def metric(labels, dec1, dec2):\n",
                "    return base_metric(labels, dec2) - base_metric(labels, dec1)\n",
                "\n",
                "metric_values = []\n",
                "num_samples = len(decisions['sys1'])\n",
                "for nb in np.arange(num_bootstraps):\n",
                "    indices = get_bootstrap_indices(num_samples, conditions=None, random_state=nb)\n",
                "#    indices = get_bootstrap_indices(num_samples, conditions=conditions, random_state=nb)\n",
                "    metric_values.append(metric(labels[indices], decisions['sys3'][indices], decisions['sys2'][indices]))\n",
                "\n",
                "print(metric(labels, decisions['sys1'], decisions['sys2']), get_conf_int(metric_values, alpha))\n"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "collapsed_sections": [],
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
